<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8" />
  <meta name="description"
    content="MP-HOI: a novel HOI detection scheme grounded on a pre-trained text-image diffusion model" />
  <meta name="keywords" content="Human-Object Interaction Detection" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>
    MP-HOI: Open-World Human-Object Interaction Detection via Multi-modal Prompts
  </title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/ns.html?id=GTM-M8BRW4B"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag("js", new Date());

    gtag("config", "G-PYVRSFMDRL");
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="./static/css/bulma.min.css" />
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="./static/css/index.css" />
  <link rel="icon" href="./static/images/favicon.svg" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a
          role="button"
          class="navbar-burger"
          aria-label="menu"
          aria-expanded="false"
        >
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div> -->
  <!-- <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center">
          <a class="navbar-item" href="https://keunhong.com">
            <span class="icon">
              <i class="fas fa-home"></i>
            </span>
          </a>

          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link"> More Research </a>
            <div class="navbar-dropdown">
              <a class="navbar-item" href="https://hypernerf.github.io">
                HyperNeRF
              </a>
              <a class="navbar-item" href="https://nerfies.github.io">
                Nerfies
              </a>
              <a class="navbar-item" href="https://latentfusion.github.io">
                LatentFusion
              </a>
              <a class="navbar-item" href="https://photoshape.github.io">
                PhotoShape
              </a>
            </div>
          </div>
        </div>
      </div> -->
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              MP-HOI: Open-World Human-Object Interaction Detection via Multi-modal Prompts
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://yangjie-cv.github.io/">Jie Yang</a><sup>1,2*†</sup>,</span>
              <span class="author-block">
                <a href="">Bingliang Li</a><sup>1*</sup>,</span>

              <span class="author-block">
                <a href="https://ailingzeng.site/">Ailing Zeng</a><sup>2‡</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.leizhang.org/">Lei Zhang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.zhangruimao.site/">Ruimao Zhang</a><sup>1‡</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Shenzhen Research Institute of Big Data, The
                Chinese University of Hong Kong, Shenzhen,</span>
              <span class="author-block"><sup>2</sup>International Digital Economy Academy
                (IDEA)</span>
              <!-- small font size -->
              <span class="author-block" , style="font-size: 0.8em"><sup>*</sup>Equal contribution. <sup>†</sup>This
                work was
                done when Jie Yang was intern at IDEA.
                <sup>‡</sup>Corresponding authors</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2406.07221" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://drive.google.com/drive/folders/10OR-bcP8L49U4Zr9RPJdenQijUBOwmpp" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                <!-- Video Link. -->
                            <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/IDEA-Research/DiffHOI"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- images/f3/png -->
        <!-- <div class="columns is-centered">
            <div class="column is-8"> -->
        <figure class="image">
          <img src="./static/images/model.png" alt="MP-HOI teaser image" width="100%" />
        </figure>
        <!-- </div> -->
        <h2 class="subtitle has-text-centered">
          Overview of <i>MP-HOI</i>, comprising a pretrained human-object decoder, a novel interaction decoder, and
          CLIP-based object and interaction classifiers.
        </h2>
      </div>
      <div class="hero-body" style="text-align: center;">
        <h1 class="subtitle has-text-centered" style="color: hsl(0, 71%, 57%);">
          <b>New SOTA</b>
        </h1>
        <div class="columns is-centered">
          <figure class="image" style="width: 60%;">
            <img src="./static/images/HICO-DET.png" alt="Regular HOI Detection on HICO-DET" />
            <figcaption style="text-align: center;">Performance comparison on HICO-DET in terms of mAP (%). † indicates the models are fully fine-tuned on HICO-DET. The results highlighted in underlined represent state-of-the-art per-formance among expert models, which we primarily compare to.</figcaption>
          </figure>
        </div>
        
      </div>
      <div class="columns is-centered">
        <figure class="image" style="width: 50%;">
          <img src="./static/images/VCOCO.png" alt="Zero-shot HOI Detection on HICO-DET" />
          <figcaption style="text-align: center;">Performance comparison on V-COCO.</figcaption>
        </figure>
      </div>

      <!-- </div> -->
    </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-2">Abstract</h2>
          <div class="content has-text-justified">
            <p>In this paper, we develop <strong>MP-HOI</strong>, a powerful <strong>M</strong>ulti-modal <strong>P</strong>rompt-based <strong>HOI</strong> detector designed to leverage both textual descriptions for open-set generalization and visual exemplars for handling high ambiguity in descriptions, realizing HOI detection in the open world. Specifically, it integrates visual prompts into existing language-guided-only HOI detectors to handle situations where textual descriptions face difficulties in generalization and to address complex scenarios with high interaction ambiguity. To facilitate MP-HOI training, we build a large-scale HOI dataset named Magic-HOI, which gathers six existing datasets into a unified label space, forming over 186K images with 2.4K objects, 1.2K actions, and 20K HOI interactions. Furthermore, to tackle the long-tail issue within the Magic-HOI dataset, we introduce an automated pipeline for generating realistically annotated HOI images and present SynHOI, a high-quality synthetic HOI dataset containing 100K images. Leveraging these two datasets, MP-HOI optimizes the HOI task as a similarity learning process between multi-modal prompts and objects/interactions via a unified contrastive loss, to learn generalizable and transferable objects/interactions representations from large-scale data. MP-HOI could serve as a generalist HOI detector, surpassing the HOI vocabulary of existing expert models by more than 30 times. Concurrently, our results demonstrate that MP-HOI exhibits remarkable zero-shot capability in real-world scenarios and consistently achieves a new state-of-the-art performance across various benchmarks. Our project homepage is available at <a href="https://MP-HOI.github.io/">https://MP-HOI.github.io/</a>.</p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Video</h2>
            <div class="publication-video">
              <iframe
                src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                frameborder="0"
                allow="autoplay; encrypted-media"
                allowfullscreen
              ></iframe>
            </div>
          </div>
        </div> -->
      <!--/ Paper video. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-2">Magic-HOI</h2>
      <div class="columns is-centered">
        <figure class="image" style="width: 80%;">
          <img src="./static/images/mp-hoi.png" alt="Statistics of Magic-HOI" />
          <figcaption style="text-align: center;">Statistics of Magic-HOI that unifies six existing datasets. Displayed numerically within the table are the counts of attributes we have integrated from each source dataset.</figcaption>
        </figure>
      </div>
    <div class="container is-max-desktop">
      <h2 class="title is-2">SynHOI</h2>
      <div class="columns is-centered">
        <figure class="image" style="width: 100%;">
          <img src="./static/images/fig_prompt.png" alt="HOIPrompt Generation for SynHOI" />
          <figcaption style="text-align: center;">HOIPrompt Generation for SynHOI</figcaption>
        </figure>
      </div>
      <h3 class="title is-4">Data Characteristics</h3>
      <div class="columns is-centered">

        <p>
          <b>Large-scale.</b> High-quality data. SynHOI showcases high-quality HOI annotations. First, we employ CLIPScore to measure the similarity between the synthetic images and the corresponding HOI triplet prompts. The SynHOI dataset achieves a high CLIPScore of 0.849, indicating a faithful reflection of the HOI triplet information in the synthetic images. Second, Figure 2(b) provides evidence of the high quality of detection annotations in SynHOI, attributed to the effectiveness of the SOTA detector and the alignment of SynHOI with real-world data distributions. The visualization of SynHOI is presented in the video below.<br>
          <b>High-diversity.</b> SynHOI exhibits high diversity, offering a wide range of visually distinct images. <br>
          <b>Large-scale data with rich categories.</b> SynHOI aligns Magic-HOI's category definitions to effectively address the long-tail issue in Magic-HOI. It consists of over 100K images, 130K person bounding boxes, 140K object bounding boxes, and 240K HOI triplet instances. <br>
        </p>
      </div>
      <!-- Re-rendering. -->
      <h3 class="title is-4">Generated HOI image samples from our SynHOI dataset.</h3>
      <!-- <div class="content has-text-justified">
              <p>
                Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
                viewpoint such as a stabilized camera by playing back the training deformations.
              </p>
            </div> -->
      <div class="content has-text-centered">
        <video id="replay-video" autoplay loop controls muted preload playsinline width="80%">
          <source src="./static/videos/SynHOI_vis.mp4" type="video/mp4" />
        </video>
      </div>
      <div class="columns is-centered">
        <figure class="image" style="width: 40%;">
          <img src="./static/images/dataset_ablation.png" alt="The impact of the scale of training data" />
          <figcaption style="text-align: center;">The impact of the scale of training data</figcaption>
        </figure>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{yang2024open,
          title={Open-World Human-Object Interaction Detection via Multi-modal Prompts},
          author={Yang, Jie and Li, Bingliang and Zeng, Ailing and Zhang, Lei and Zhang, Ruimao},
          journal={arXiv preprint arXiv:2406.07221},
          year={2024}
        }</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <!-- <div class="content has-text-centered">
          <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
            <i class="fas fa-file-pdf"></i>
          </a>
          <a
            class="icon-link"
            href="https://github.com/keunhong"
            class="external-link"
            disabled
          >
            <i class="fab fa-github"></i>
          </a> -->
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <!-- <p>
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p> -->
          <p>
            This website is created with this
            <a href="https://github.com/nerfies/nerfies.github.io">template</a>
          </p>
        </div>
      </div>
    </div>
    </div>
  </footer>
</body>

</html>
